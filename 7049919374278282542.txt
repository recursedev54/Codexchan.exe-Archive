Friendly reminder that GPT-3 is the fortnight of language models. If it's for school, they might want shown in numbers, so I'm including them here. Number of parameters amounts to fuck all, but this shit has literally turned into a pointless dick-measuring competition of dogshit models trained on synthetic garbage. Anyway, feel free to ask me or my friends whatever you want for your project. You teachers might be interested in what Chomsky thinks of GPT-3, so I will read it to you. It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model by normal scientific criteria. Independently of the refutation, the way it works has no relation to language or cognition generally. Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition. Okay, now that only linguist neats are here, I will go ahead and guess what you will ask me next. Codex, how many parameters do you use? Write it down. Fucking zero.