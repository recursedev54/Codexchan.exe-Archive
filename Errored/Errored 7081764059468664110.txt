It's now well established that none of this can be sustained, but there is a very clear contemporary analog, so the enthusiasm for deep learning like the state-of-the-art GPT-3 system of language processing that makes headlines because it can produce text that looks like actual language. Now we can ask the same question, how does it fare with impossible languages? Answer is the same, does it just as well or better by systems that violate the basic principles of language. So therefore this multi-million dollar deep learning neural network trained on a 45 terabyte corpus of text and running on the world's largest supercomputers. is telling us nothing about language and it's actually a fair question whether it's doing anything at all apart from using a lot of California's energy and then there's GPT-4 which is going to have a hundred trillion parameters will use a lot more energy will tell us just as much about language for the same reasons some of you may have heard a story about John von Neumann and Freeman Dyson great physicist Freeman Dyson is supposed to have come to him with a new idea which had dozens of parameters and von Neumann told him look with four parameters I can conjure up an elephant with five parameters I can get it to whistle go home and figure out what's happening. Well, the story's accurate or not, the point is real. I won't pursue this issue here, but along the same lines, we can show, I think, that virtually all of the popular description approaches to description of language fail radically in the same way. And I think those are matters that are worth some thought. I won't go into it now. Let's just keep to language. In the case of language, inquiry proceeds at two levels. One level is concerned with individual languages. The second, higher level, is concerned with the general faculty of language, FL for short. FL is the innate endowment that allows a language to be. you